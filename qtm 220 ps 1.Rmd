---
title: "qtm 220 ps 1"
output: html_document
date: "2023-01-31"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
emdata <- read.csv("EMdataFull2016.csv")
```
```{r}
# problem 2 part 1
y <- emdata$CLlib
x <- emdata$SCscore
# to find b1 using formula b1 = cov(x,y)/var(x)
b1 = cov(x,y)/var(x)
b1
# b1 is 43.155
# once we have b1, we find b0 by plugging b1 back to the equation b0 = y_mean - b1(x_mean)
b0 = mean(y)- b1*mean(x)
b0
# b0 is 27.559
```
```{r}
# problem 2 part 2
prediction_y = b1*x + b0
prediction_y
residuals_e = y - prediction_y
table <-data.frame(prediction_y,residuals_e)
# a table showing the predictions of y and residuals e
table
```
```{r}
# problem 2 part 3
sum_e = sum(residuals_e)
round(sum_e, 0)
#when we round the sum of residuals, it equals zero
```
```{r}
# problem 2 part 4
cov_xe = cov(x,residuals_e)
round(cov_xe, 0)
#when we round the covariance of x and residuals, it equals zero
```
```{r}
# problem 2 part 5
cov_ye = cov(prediction_y, residuals_e)
round(cov_ye, 0)
#when we round the covariance of predicted y and residuals, it equals zero
```

```{r}
# problem 2 part 6
part_6 <- round(var(prediction_y)/var(y),2)
```
```{r}
#problem 2 part 7
part_7 <- round(cor(x,y)^2,2)
part_6 == part_7
#part 6 is equal to part 7
```
```{r}
#problem 2 part 8
part_8 <- cor(x,y) * (sd(y)/sd(x))
round(b1,2) == round(part_8,2)
#part 8 equals to b1 calculated in part 1
```


```{r}
#problem 3 part 1
mod.1 = lm(CLlib~1, data = emdata)
mod.1
# interpretation: in this intercept-only model, the intercept 50.86 is the sample mean of CLlib
mean_CLlib <- round(mean(emdata$CLlib),2)
50.86 == mean_CLlib
# mean of civil liberties equals the intercept
library(tidyverse)
predicted <- data_frame (CLlib = predict(mod.1))
ggplot(data = emdata, aes(x = seq_along(CLlib), y = CLlib)) +
  geom_point(color = "steelblue", alpha = 0.2, size = 2) +
  geom_line(color = "red", data = predicted, aes(x = seq_along(CLlib), y = CLlib), lwd = 1)+
              theme_bw()
#calculating R squared
var(predicted)/var(emdata$CLlib)
# R squared is zero
```
```{r}
#problem 3 part 2
mod.2 = lm(CLlib~ -1 + SCscore, data = emdata)
summary(mod.2)
# R squared is 0.8579, meaning that 85.79% of the variation can be explained by the model
SCscore_seq <- seq(0,1,0.01)
predicted2 <- predict(mod.2, newdata = data.frame(SCscore = SCscore_seq))
predicted_df <- data.frame(SCscore = SCscore_seq, CLlib = predicted2)
ggplot(data = emdata, aes(x= SCscore, y = CLlib)) +
  geom_point(color = "steelblue", alpha = 0.2, size = 2) +
  geom_line(color = "red", data = predicted_df, aes(x = SCscore, y = CLlib), lwd = 1) +
  theme_bw()
# In the slope-only model, we do not have intercept. The coefficient of SCscore is 80.975. The result shows that every one unit of increase in SCscore, CLlib increase by 80.975%
```
```{r}
#problem 3 part 3
mod.3 = lm(CLlib~ 1 + SCscore, data = emdata)
summary(mod.3)
#R squared is 0.4515, meaning that 85.79% of the variation in data can be explained by the model
predicted3 <- predict(mod.3, newdata = data.frame(SCscore = SCscore_seq))
predicted_df1 <- data.frame(SCscore = SCscore_seq, CLlib = predicted3)
ggplot(data = emdata, aes(x = SCscore, y = CLlib)) + 
  geom_point(color="steelblue", alpha = 0.2, size = 2) +
  geom_line(color="red", data = predicted_df1, lwd = 1) +
  theme_bw()
# The intercept is 27.559, the coefficient of SCscore is 43.115. the result shows with zero Scscore, the CLlib is 27.559 and that every one unit of increase in SCscore, CLlib increase by 43.115%
# slopes in parts 2 and 3 are different because we include a intercept in part 3. Model 2 is better because it has a larger R squared. 
```


